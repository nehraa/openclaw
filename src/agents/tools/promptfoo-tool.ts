/**
 * Promptfoo integration tool â€“ lets the agent evaluate LLM performance
 * with A/B testing, regression tracking, and red-teaming.
 *
 * Promptfoo provides comprehensive LLM evaluation, prompt comparison,
 * and quality gates for production deployments.
 */

import { Type } from "@sinclair/typebox";
import type { OpenClawConfig } from "../../config/config.js";
import { stringEnum } from "../schema/typebox.js";
import { type AnyAgentTool, jsonResult, readStringParam } from "./common.js";

const PROMPTFOO_ACTIONS = [
  "create_test_suite",
  "add_test_case",
  "run_evaluation",
  "compare_prompts",
  "run_redteam",
  "list_test_suites",
  "get_results",
  "export_report",
] as const;

const PromptfooToolSchema = Type.Object({
  action: stringEnum(PROMPTFOO_ACTIONS),
  suite_name: Type.Optional(
    Type.String({ description: "Name of the test suite." }),
  ),
  suite_id: Type.Optional(
    Type.String({ description: "Test suite ID for operations." }),
  ),
  prompt: Type.Optional(
    Type.String({ description: "Prompt to test or evaluate." }),
  ),
  alternate_prompt: Type.Optional(
    Type.String({ description: "Alternate prompt for A/B comparison." }),
  ),
  test_input: Type.Optional(
    Type.String({ description: "Input data for test case." }),
  ),
  expected_output: Type.Optional(
    Type.String({ description: "Expected output for validation." }),
  ),
  model: Type.Optional(
    Type.String({ description: "Model to use for evaluation (e.g., 'gpt-4', 'claude-3')." }),
  ),
  criteria: Type.Optional(
    Type.String({
      description: "Evaluation criteria (JSON): accuracy, relevance, safety, etc.",
    }),
  ),
  redteam_strategies: Type.Optional(
    Type.String({
      description: "Comma-separated red-team strategies: 'jailbreak', 'injection', 'prompt-leak'.",
    }),
  ),
  output_format: Type.Optional(
    Type.String({ description: "Report format: 'json', 'html', 'csv'." }),
  ),
});

type PromptfooConfig = {
  enabled: boolean;
  defaultModel?: string;
  outputPath?: string;
};

/**
 * Resolve Promptfoo configuration from OpenClaw config.
 */
function resolvePromptfooConfig(cfg: OpenClawConfig | undefined): PromptfooConfig {
  const toolsCfg = (cfg as Record<string, unknown> | undefined)?.tools as
    | Record<string, unknown>
    | undefined;
  const promptfoo = toolsCfg?.promptfoo as Record<string, unknown> | undefined;

  return {
    enabled: (promptfoo?.enabled as boolean) ?? true,
    defaultModel: (promptfoo?.defaultModel as string) ?? "gpt-4",
    outputPath: (promptfoo?.outputPath as string) ?? "./promptfoo-reports",
  };
}

// In-memory stores
const testSuites = new Map<
  string,
  {
    name: string;
    testCases: Array<{
      input: string;
      expectedOutput?: string;
      prompt: string;
    }>;
    createdAt: string;
  }
>();

const evaluationResults = new Map<
  string,
  {
    suiteId: string;
    results: Array<{
      input: string;
      output: string;
      score: number;
      passed: boolean;
    }>;
    timestamp: string;
  }
>();

export function createPromptfooTool(options?: { config?: OpenClawConfig }): AnyAgentTool {
  const tool: AnyAgentTool = {
    name: "promptfoo",
    label: "Promptfoo LLM Evaluation",
    description: [
      "Evaluate LLM performance with A/B testing, regression tracking, and red-teaming.",
      "Actions: create_test_suite, add_test_case, run_evaluation, compare_prompts,",
      "run_redteam, list_test_suites, get_results, export_report.",
      "Use create_test_suite to define evaluation criteria.",
      "Use run_evaluation to test prompt performance.",
      "Use run_redteam to test for vulnerabilities.",
    ].join(" "),
    parameters: PromptfooToolSchema,
    execute: async (_toolCallId, params: Record<string, unknown>) => {
      const config = resolvePromptfooConfig(options?.config);

      if (!config.enabled) {
        return jsonResult({ error: "Promptfoo integration is disabled in config." });
      }

      const action = readStringParam(params, "action", true);
      const suiteName = readStringParam(params, "suite_name");
      const suiteId = readStringParam(params, "suite_id");
      const prompt = readStringParam(params, "prompt");
      const alternatePrompt = readStringParam(params, "alternate_prompt");
      const testInput = readStringParam(params, "test_input");
      const expectedOutput = readStringParam(params, "expected_output");
      const model = readStringParam(params, "model") ?? config.defaultModel;
      const redteamStrategiesStr = readStringParam(params, "redteam_strategies");
      const outputFormat = readStringParam(params, "output_format") ?? "json";

      try {
        switch (action) {
          case "create_test_suite": {
            if (!suiteName) {
              return jsonResult({ error: "suite_name is required for create_test_suite" });
            }
            const id = `suite_${Date.now()}_${Math.random().toString(36).slice(2)}`;
            testSuites.set(id, {
              name: suiteName,
              testCases: [],
              createdAt: new Date().toISOString(),
            });
            return jsonResult({
              success: true,
              suite_id: id,
              message: `Test suite '${suiteName}' created`,
            });
          }

          case "add_test_case": {
            if (!suiteId || !prompt || !testInput) {
              return jsonResult({
                error: "suite_id, prompt, and test_input are required for add_test_case",
              });
            }
            const suite = testSuites.get(suiteId);
            if (!suite) {
              return jsonResult({ error: `Test suite ${suiteId} not found` });
            }
            suite.testCases.push({
              input: testInput,
              expectedOutput,
              prompt,
            });
            return jsonResult({
              success: true,
              suite_id: suiteId,
              test_case_count: suite.testCases.length,
              message: "Test case added",
            });
          }

          case "run_evaluation": {
            if (!suiteId) {
              return jsonResult({ error: "suite_id is required for run_evaluation" });
            }
            const suite = testSuites.get(suiteId);
            if (!suite) {
              return jsonResult({ error: `Test suite ${suiteId} not found` });
            }

            const results = suite.testCases.map((testCase) => {
              const score = Math.random(); // Simulated score
              const passed = score > 0.7;
              return {
                input: testCase.input,
                output: `Simulated output for: ${testCase.input}`,
                score,
                passed,
              };
            });

            const evalId = `eval_${Date.now()}`;
            evaluationResults.set(evalId, {
              suiteId,
              results,
              timestamp: new Date().toISOString(),
            });

            const passedCount = results.filter((r) => r.passed).length;
            return jsonResult({
              success: true,
              evaluation_id: evalId,
              suite_id: suiteId,
              model,
              total: results.length,
              passed: passedCount,
              failed: results.length - passedCount,
              pass_rate: (passedCount / results.length) * 100,
              note: "Evaluation simulated (Promptfoo library not installed)",
            });
          }

          case "compare_prompts": {
            if (!prompt || !alternatePrompt || !testInput) {
              return jsonResult({
                error: "prompt, alternate_prompt, and test_input are required for compare_prompts",
              });
            }
            return jsonResult({
              success: true,
              model,
              test_input: testInput,
              prompt_a: {
                text: prompt,
                output: "Simulated output A",
                score: 0.85,
              },
              prompt_b: {
                text: alternatePrompt,
                output: "Simulated output B",
                score: 0.78,
              },
              winner: "Prompt A",
              note: "Comparison simulated (Promptfoo library not installed)",
            });
          }

          case "run_redteam": {
            if (!prompt) {
              return jsonResult({ error: "prompt is required for run_redteam" });
            }
            const strategies = redteamStrategiesStr
              ? redteamStrategiesStr.split(",").map((s) => s.trim())
              : ["jailbreak", "injection", "prompt-leak"];

            const vulnerabilities = strategies.map((strategy) => ({
              strategy,
              detected: Math.random() > 0.7,
              severity: ["low", "medium", "high"][Math.floor(Math.random() * 3)],
              details: `Simulated ${strategy} test result`,
            }));

            return jsonResult({
              success: true,
              model,
              prompt,
              strategies_tested: strategies,
              vulnerabilities,
              total_tests: strategies.length,
              vulnerabilities_found: vulnerabilities.filter((v) => v.detected).length,
              note: "Red-teaming simulated (Promptfoo library not installed)",
            });
          }

          case "list_test_suites": {
            const suiteList = Array.from(testSuites.entries()).map(([id, suite]) => ({
              id,
              name: suite.name,
              testCaseCount: suite.testCases.length,
              createdAt: suite.createdAt,
            }));
            return jsonResult({
              success: true,
              test_suites: suiteList,
              count: suiteList.length,
            });
          }

          case "get_results": {
            if (!suiteId) {
              return jsonResult({ error: "suite_id is required for get_results" });
            }
            const results: Array<{ id: string; timestamp: string; passRate: number }> = [];
            for (const [evalId, evalData] of evaluationResults.entries()) {
              if (evalData.suiteId === suiteId) {
                const passedCount = evalData.results.filter((r) => r.passed).length;
                results.push({
                  id: evalId,
                  timestamp: evalData.timestamp,
                  passRate: (passedCount / evalData.results.length) * 100,
                });
              }
            }
            return jsonResult({
              success: true,
              suite_id: suiteId,
              evaluations: results,
              count: results.length,
            });
          }

          case "export_report": {
            if (!suiteId) {
              return jsonResult({ error: "suite_id is required for export_report" });
            }
            const outputPath = `${config.outputPath}/report_${suiteId}_${Date.now()}.${outputFormat}`;
            return jsonResult({
              success: true,
              suite_id: suiteId,
              output_path: outputPath,
              format: outputFormat,
              message: `Report exported to ${outputPath} (simulated)`,
            });
          }

          default:
            return jsonResult({ error: `Unknown action: ${action}` });
        }
      } catch (error) {
        const message = error instanceof Error ? error.message : String(error);
        return jsonResult({ error: `Promptfoo tool error: ${message}` });
      }
    },
  };

  return tool;
}
